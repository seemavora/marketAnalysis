{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import boxcox, skew\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacForNumClusters(X,n):\n",
    "    for cluster_num in range(2,n):\n",
    "        hac(X,cluster_num)\n",
    "        \n",
    "def hac(X,cluster_num):\n",
    "    hac = AgglomerativeClustering(n_clusters = cluster_num)\n",
    "    labels = hac.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    print('clus {}: {}, {}'.format(cluster_num, sil, db))\n",
    "    return labels\n",
    "        \n",
    "def cluster(filepath, num_clus, mult=False):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.drop(['Bios'], axis=1)\n",
    "    \n",
    "    X = df.to_numpy()\n",
    "    #minmax scaling\n",
    "    X_minmax = MinMaxScaler().fit_transform(X)\n",
    "    \n",
    "    #dimensionality reduction\n",
    "    pca = PCA(n_components = 0.99)\n",
    "    X = pca.fit_transform(X_minmax)\n",
    "    \n",
    "    if mult:\n",
    "        hacForNumClusters(X,num_clus)\n",
    "    else:\n",
    "        labels = hac(X,num_clus)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset 1\n",
      "clus 2: 0.12235658968599077, 2.324817108471529\n",
      "clus 3: 0.12310580948201447, 1.953528913385784\n",
      "clus 4: 0.14057097283298264, 1.7832065534952526\n",
      "clus 5: 0.13062037616800862, 1.6911714070371886\n",
      "clus 6: 0.144702792506051, 1.554390151635028\n",
      "clus 7: 0.15655025249360324, 1.4375361542116012\n",
      "clus 8: 0.1735340949176274, 1.3643807760938174\n",
      "clus 9: 0.177504533701653, 1.2881069287151043\n",
      "clus 10: 0.17648241744624052, 1.2260451189603734\n",
      "clus 11: 0.17667364525751203, 1.1755872535404062\n",
      "clus 12: 0.17524941512017206, 1.018800857726462\n",
      "clus 13: 0.18310270790846178, 0.9811657609988056\n",
      "clus 14: 0.18475638852802329, 0.9554377625747816\n",
      "clus 15: 0.1820600880774312, 0.927854062336475\n",
      "clus 16: 0.18720701361897127, 0.8595129379261999\n",
      "clus 17: 0.19334004554133216, 0.8181948486025262\n",
      "clus 18: 0.18702398101729764, 0.7386998383250655\n",
      "clus 19: 0.1817981849052378, 0.6670735684725386\n",
      "clus 20: 0.17705800984855707, 0.6292237128534461\n",
      "clus 21: 0.17437849054190713, 0.581125323251557\n",
      "clus 22: 0.1658747367967631, 0.5362195798349702\n",
      "clus 23: 0.15381158447468332, 0.48788010323694164\n",
      "clus 24: 0.14367529346219016, 0.439759549892742\n",
      "clus 25: 0.12751440014584886, 0.3892491093704797\n",
      "clus 26: 0.11344875130219628, 0.35245796309954514\n",
      "clus 27: 0.09499021182155486, 0.32250466190738897\n",
      "clus 28: 0.0784163525544992, 0.284434425841381\n",
      "clus 29: 0.06128182237492563, 0.24054773790891562\n"
     ]
    }
   ],
   "source": [
    "print('dataset 1')\n",
    "labels1 = cluster('./TEST1.csv',30,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset 1\nclus 3: 0.12310580948201447, 1.953528913385784\n"
     ]
    }
   ],
   "source": [
    "print('dataset 1')\n",
    "labels1 = cluster('./TEST1.csv',3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.mkdir(os.getcwd() + '/' + someWord)\n",
    "def please(filepath, labels, n, someWord):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['cluster #'] = labels\n",
    "    #os.mkdir(os.getcwd() + '/' + someWord)\n",
    "    \n",
    "    \n",
    "    for cluster in range(n):\n",
    "        yes = df[df['cluster #'] == cluster]\n",
    "        filename = '{}/{}.csv'.format(someWord, str(cluster))\n",
    "        yes.to_csv(filename)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "please('./TEST1.csv',labels1,3,'woohoo1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-ac70f1c67bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mno_punct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mno_punct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Bios'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Bios'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_punct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Bios'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Bios'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def remove_punct(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "df['Bios'] = df['Bios'].apply(lambda x: remove_punct(x))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['Bios'] = df['Bios'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "df['Bios'] = df['Bios'].apply(lambda x: remove_stopwords(x))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "df['Bios'] = df['Bios'].apply(lambda x: word_lemmatizer(x))\n",
    "stemmer = PorterStemmer()\n",
    "def word_stemmer(text):\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text\n",
    "\n",
    "df['Bios'] = df['Bios'].apply(lambda x: word_stemmer(x))\n",
    "print(df.head())\n",
    "#TF-IDF vectorizer\n",
    "tfv = TfidfVectorizer(stop_words = \"english\",ngram_range = (1,1))\n",
    "#transform\n",
    "vec_text = tfv.fit_transform(df.Bios)\n",
    "#returns a list of words.\n",
    "words = tfv.get_feature_names()\n",
    "\n",
    "kmeans = KMeans(n_clusters = 10, n_init = 17, tol = 0.01, max_iter = 200)\n",
    "#fit the data \n",
    "kmeans.fit(vec_text)\n",
    "#this loop transforms the numbers back into words\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_cluster_for_cluster_spec(dir_spec):\n",
    "    dirname = '{}/{}'.format(os.getcwd(),dir_spec)\n",
    "    for filename in os.listdir(dirname):\n",
    "        labels = nlp_clus('{}/{}'.format(dirname, filename), 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9929731916162744\nclus 10: 0.12360022842993289, 0.4967414371115811\n0.9927527424286925\nclus 10: 0.1345629286265202, 0.2960262879519318\n0.990804191706378\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Cannot extract more clusters than samples: 10 clusters where given for a tree with 6 leaves.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-831913399b68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp_cluster_for_cluster_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./woohoo1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-2ac31db822fc>\u001b[0m in \u001b[0;36mnlp_cluster_for_cluster_spec\u001b[1;34m(dir_spec)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdirname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdir_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_clus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-164c3ccbacda>\u001b[0m in \u001b[0;36mnlp_clus\u001b[1;34m(filename, n, desc)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-fe69457627d0>\u001b[0m in \u001b[0;36mcluster\u001b[1;34m(filepath, num_clus, mult)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mhacForNumClusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_clus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_clus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-fe69457627d0>\u001b[0m in \u001b[0;36mhac\u001b[1;34m(X, cluster_num)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcluster_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mhac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdavies_bouldin_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    923\u001b[0m             \u001b[0mCluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m         \"\"\"\n\u001b[1;32m--> 925\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[1;31m# Cut the tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcompute_full_tree\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m             self.labels_ = _hc_cut(self.n_clusters_, self.children_,\n\u001b[0m\u001b[0;32m    898\u001b[0m                                    self.n_leaves_)\n\u001b[0;32m    899\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36m_hc_cut\u001b[1;34m(n_clusters, children, n_leaves)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \"\"\"\n\u001b[0;32m    656\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mn_leaves\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         raise ValueError('Cannot extract more clusters than samples: '\n\u001b[0m\u001b[0;32m    658\u001b[0m                          \u001b[1;34m'%s clusters where given for a tree with %s leaves.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m                          % (n_clusters, n_leaves))\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot extract more clusters than samples: 10 clusters where given for a tree with 6 leaves."
     ]
    }
   ],
   "source": [
    "nlp_cluster_for_cluster_spec('./woohoo1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}