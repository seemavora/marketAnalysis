{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import boxcox, skew\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacForNumClusters(X,n):\n",
    "    for cluster_num in range(2,n):\n",
    "        hac(X,cluster_num)\n",
    "        \n",
    "def hac(X,cluster_num):\n",
    "    hac = AgglomerativeClustering(n_clusters = cluster_num)\n",
    "    labels = hac.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    print('clus {}: {}, {}'.format(cluster_num, sil, db))\n",
    "    return labels\n",
    "        \n",
    "def cluster(filepath, num_clus, mult=False):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.drop(['Bios'], axis=1)\n",
    "    \n",
    "    X = df.to_numpy()\n",
    "    #minmax scaling\n",
    "    X_minmax = MinMaxScaler().fit_transform(X)\n",
    "    \n",
    "    #dimensionality reduction\n",
    "    pca = PCA(n_components = 0.99)\n",
    "    X = pca.fit_transform(X_minmax)\n",
    "    \n",
    "    if mult:\n",
    "        hacForNumClusters(X,num_clus)\n",
    "    else:\n",
    "        labels = hac(X,num_clus)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset 1\n",
      "clus 2: 0.13110979245482507, 2.523718796851537\n",
      "clus 3: 0.11350183171234067, 2.27576431436099\n",
      "clus 4: 0.14534076344714092, 2.3041647128304996\n",
      "clus 5: 0.12320377464402425, 2.1213396915745486\n",
      "clus 6: 0.12751792322631578, 2.020743046093323\n",
      "clus 7: 0.14704446279755481, 2.077890034739056\n",
      "clus 8: 0.1631457150382885, 2.084184944744435\n",
      "clus 9: 0.15048420188500422, 2.1128459750676942\n",
      "clus 10: 0.14372754804549123, 2.0748823733230566\n",
      "clus 11: 0.1296749080357924, 2.0773981020704677\n",
      "clus 12: 0.118329640964038, 2.1589801052345945\n",
      "clus 13: 0.11731439825403481, 2.2308298722974502\n",
      "clus 14: 0.11125248853191574, 2.2778935785983645\n",
      "clus 15: 0.10580318411999988, 2.277235846905591\n",
      "clus 16: 0.10658404563054132, 2.2952460158294894\n",
      "clus 17: 0.10665230251282805, 2.2283954064427185\n",
      "clus 18: 0.10697184244834389, 2.183186921269965\n",
      "clus 19: 0.10488701755371804, 2.215887428345672\n",
      "clus 20: 0.10442376431722256, 2.226255295150083\n",
      "clus 21: 0.10440408669174742, 2.177423202243742\n",
      "clus 22: 0.10532728064068718, 2.172852844788452\n",
      "clus 23: 0.10717349224265246, 2.1230886199540957\n",
      "clus 24: 0.10742674841966925, 2.094985659739994\n",
      "clus 25: 0.10805810303468043, 2.048983017403046\n",
      "clus 26: 0.10878109324675704, 2.0080926182032335\n",
      "clus 27: 0.10973176893663786, 1.9810487775975847\n",
      "clus 28: 0.11015396770417955, 1.9664597319521882\n",
      "clus 29: 0.11155354148878235, 1.948237973542274\n"
     ]
    }
   ],
   "source": [
    "print('dataset 1')\n",
    "labels1 = cluster('org.csv',30,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset 1\nclus 3: 0.11350183171234067, 2.27576431436099\n"
     ]
    }
   ],
   "source": [
    "print('dataset 1')\n",
    "labels1 = cluster('org.csv',3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.mkdir(os.getcwd() + '/' + someWord)\n",
    "def please(filepath, labels, n, someWord):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['cluster #'] = labels\n",
    "    #os.mkdir(os.getcwd() + '/' + someWord)\n",
    "    \n",
    "    \n",
    "    for cluster in range(n):\n",
    "        yes = df[df['cluster #'] == cluster]\n",
    "        filename = '{}/{}.csv'.format(someWord, str(cluster))\n",
    "        yes.to_csv(filename)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "please('org.csv',labels1,3,'woohoo1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def clean(filename, n, desc=False):\n",
    "    df = pd.read_csv(filename)\n",
    "    text = df['Bios']\n",
    "    df = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = text.apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "   \n",
    "    text = [w for w in text if w not in stopwords.words('english')]\n",
    "    \n",
    "    # stemmer = PorterStemmer()\n",
    "    # stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "   \n",
    "    labels = cluster(filename,n,False)\n",
    "    return labels\n",
    "    # df['Bios'] = df['Bios'].apply(lambda x: word_lemmatizer(x))\n",
    "    # stemmer = PorterStemmer()\n",
    "    # def word_stemmer(text):\n",
    "    #     stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    #     return stem_text\n",
    "\n",
    "    # df['Bios'] = df['Bios'].apply(lambda x: word_stemmer(x))\n",
    "    # print(df.head())\n",
    "    # #TF-IDF vectorizer\n",
    "    # tfv = TfidfVectorizer(stop_words = \"english\",ngram_range = (1,1))\n",
    "    # #transform\n",
    "    # vec_text = tfv.fit_transform(df.Bios)\n",
    "    # #returns a list of words.\n",
    "    # words = tfv.get_feature_names()\n",
    "\n",
    "    # kmeans = KMeans(n_clusters = 10, n_init = 17, tol = 0.01, max_iter = 200)\n",
    "    # #fit the data \n",
    "    # kmeans.fit(vec_text)\n",
    "    # #this loop transforms the numbers back into words\n",
    "    # common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "    # for num, centroid in enumerate(common_words):\n",
    "    #     print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n",
    "\n",
    "    \n",
    "# print(clean('./woohoo1/0.csv', 10, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_clus(filename, n, desc=False):\n",
    "    df = pd.read_csv(filename)\n",
    "    # Instantiating the Vectorizer, experimenting with both\n",
    "    vectorizer = CountVectorizer()\n",
    "    #vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fitting the vectorizer to the Bios\n",
    "    x = vectorizer.fit_transform(df['Bios'])\n",
    "\n",
    "    # Creating a new DF that contains the vectorized words\n",
    "    df_wrds = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "    # Concating the words DF with the original DF\n",
    "    new_df = pd.concat([df, df_wrds], axis=1)\n",
    "\n",
    "    # Dropping the Bios because it is no longer needed in place of vectorization\n",
    "    new_df.drop('Bios', axis=1, inplace=True)\n",
    "\n",
    "    # Instantiating PCA\n",
    "    pca = PCA()\n",
    "\n",
    "    # Fitting and Transforming the DF\n",
    "    df_pca = pca.fit_transform(new_df)\n",
    "\n",
    "    # Reducing the dataset to the number of features determined before\n",
    "    pca = PCA(n_components=0.99)\n",
    "\n",
    "    # Fitting and transforming the dataset to the stated number of features and creating a new DF\n",
    "    df_pca = pca.fit_transform(new_df)\n",
    "\n",
    "    # Seeing the variance ratio that still remains after the dataset has been reduced\n",
    "    print(pca.explained_variance_ratio_.cumsum()[-1])\n",
    "    \n",
    "    labels = cluster(filename,n,False)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_cluster_for_cluster_spec(dir_spec):\n",
    "    dirname = '{}/{}'.format(os.getcwd(),dir_spec)\n",
    "    for filename in os.listdir(dirname):\n",
    "        labels = nlp_clus('{}/{}'.format(dirname, filename), 30, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9983076104672759\nclus 30: 0.14262166959225042, 1.5229368826825644\n0.9981959755423443\nclus 30: 0.16966916009567212, 1.168422746156556\n0.9981118462087376\nclus 30: 0.2032303300255138, 1.1252443949886397\n1.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Cannot extract more clusters than samples: 30 clusters where given for a tree with 4 leaves.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-831913399b68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp_cluster_for_cluster_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./woohoo1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-74-2e79123b73a1>\u001b[0m in \u001b[0;36mnlp_cluster_for_cluster_spec\u001b[1;34m(dir_spec)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdirname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdir_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_clus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-73-39c3b8e03975>\u001b[0m in \u001b[0;36mnlp_clus\u001b[1;34m(filename, n, desc)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-fe69457627d0>\u001b[0m in \u001b[0;36mcluster\u001b[1;34m(filepath, num_clus, mult)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mhacForNumClusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_clus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_clus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-fe69457627d0>\u001b[0m in \u001b[0;36mhac\u001b[1;34m(X, cluster_num)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcluster_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mhac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdavies_bouldin_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    923\u001b[0m             \u001b[0mCluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m         \"\"\"\n\u001b[1;32m--> 925\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[1;31m# Cut the tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcompute_full_tree\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m             self.labels_ = _hc_cut(self.n_clusters_, self.children_,\n\u001b[0m\u001b[0;32m    898\u001b[0m                                    self.n_leaves_)\n\u001b[0;32m    899\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36m_hc_cut\u001b[1;34m(n_clusters, children, n_leaves)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \"\"\"\n\u001b[0;32m    656\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mn_leaves\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         raise ValueError('Cannot extract more clusters than samples: '\n\u001b[0m\u001b[0;32m    658\u001b[0m                          \u001b[1;34m'%s clusters where given for a tree with %s leaves.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m                          % (n_clusters, n_leaves))\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot extract more clusters than samples: 30 clusters where given for a tree with 4 leaves."
     ]
    }
   ],
   "source": [
    "nlp_cluster_for_cluster_spec('./woohoo1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}