{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7f956853190829b392c62832bb8f0ad9f910ce06bda05f24e57857f8ee2917c7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hey', 'there', 'this', 'sample', 'review', 'happens', 'blah', 'contain', 'happened', 'punctuation', 'universal', 'right', 'right', 'contained']\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seemavora/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/seemavora/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "import string\n",
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Return the cleaned text as a list of words\n",
    "    4. Remove words\n",
    "    '''\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "    return [stemmer.lemmatize(word) for word in nopunc]\n",
    "\n",
    "#testing the function with a sample text#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['voiceover', 'talent', 'voice', 'actor', 'radio', 'guy', 'year', 'my', 'wife', 'best', 'friend', 'i', 'love', 'soup']\n",
      "['south', 'florida', 'premiere', 'nonprofit', 'cinema', 'best', 'independent', 'foreign', 'cult', 'classic', 'film', 'digitally', 'restored', 'mm', 'mm']\n",
      "['follow', 'u', 'best', 'whats', 'happening', 'cycling', 'biking', 'around', 'world', 'chika', 'chikaaaaaaa', 'ka', 'bro']\n",
      "['filmmaker', 'writer', 'activist', 'aspiring', 'actor', 'i', 'make', 'film', 'and', 'video', 'that', 'make', 'people', 'think', 'i', 'ask', 'the', 'question', 'you', 'don‚Äôt', 'have', 'the', 'cojones', 'to', 'ask']\n",
      "['dedicated', 'student', 'beautiful', 'game', 'head', 'coach', 'college', 'desert', 'men', 'soccer', 'christ', 'canine', 'coffee', 'enthusiast', 'ao']\n",
      "['in', 'business', 'mastering', 'energy', 'authentic', 'empowered', 'self', 'create', 'something', 'change', 'world']\n",
      "['southern', 'california‚Äôs', 'premier', 'electric', 'scooter', 'store', 'contact', 'u', 'saleselectriccityridesnet']\n",
      "['fast', 'friendly', 'roll', 'dumpster', 'company', 'geared', 'towards', 'residential', 'demand', 'cubic', 'yard', 'dumpster']\n",
      "['the', 'jp', 'strategy', 'team', 'provides', 'community', 'outreach', 'communication', 'engagement', 'strategy', 'political', 'campaign', 'nonprofit', 'organization', 'across', 'country']\n",
      "['i‚Äôm', 'lonely', 'bitch']\n",
      "['love', 'nature', 'flower', 'landscape', 'wildlife', 'no', 'dm', 'god', 'blez', 'everyoneüòá']\n",
      "['bake']\n",
      "['actress', 'activist', 'community', 'leader', 'entrepreneur', 'jpstrategiesco', 'cofounder', 'blackgirlshike', 'coblackarts', 'ig', 'portiaprescottco', 'brownuniversity', 'aka']\n",
      "['–Ω–∞', '–∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã', '–∏–¥—É', '–Ω–æ', '–Ω–µ', '—Å', '—Å–æ–≤–µ—Å—Ç—å—é', '–∏', '–Ω–µ', '—Å', '–¥—É—à–æ–π']\n",
      "['jesus', '‚úùÔ∏è', 'lover', 'wife', 'üë®\\u200düë©\\u200düëß', 'mom', 'bos', 'üíãüìø', 'lady', 'girl', 'üéÆüïπ', 'gamer', 'turtle', 'üê¢', 'obsessed']\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a4298dd2cb93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-8eca6d77e677>\u001b[0m in \u001b[0;36mtext_process\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m     '''\n\u001b[1;32m     18\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mnopunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mnopunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnopunc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnopunc\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnopunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./twitterBios.csv')\n",
    "bios = df['bio']\n",
    "for i in range (len(bios)):\n",
    "    print(text_process(bios[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "vec.fit(df['bio'].values.astype('U'))\n",
    "features = vec.transform(df['bio'].values.astype('U'))\n",
    "# features = vec.transform(df['bio'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "clust = KMeans(init='k-means++', n_clusters=5, n_init = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "clust.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = clust.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster Labels'] = clust.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitterBioClust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustBios = pd.read_csv('twitterBioClust.csv')\n",
    "clustBios[['ClusterLabels'] == 1]"
   ]
  }
 ]
}